---
phase: 03-ai-processing-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - src/lib/ai/pipeline.ts
  - src/lib/github/sync.ts
  - src/app/api/admin/sync/route.ts
  - src/app/api/admin/sync/cron/route.ts
autonomous: true

must_haves:
  truths:
    - "After a sync completes, the AI pipeline runs automatically on the changeset"
    - "Pipeline creates new articles with content, technical view, file links, and DB table mappings"
    - "Pipeline updates existing articles using merge strategy when they have human edits"
    - "Pipeline freely overwrites AI-only articles without merge"
    - "AI receives full category tree and article index as context during pipeline execution"
    - "Categories are created only when no existing category fits"
    - "Article file links and DB table mappings are populated from AI analysis output"
    - "Sync lock remains held until AI pipeline completes (no race condition)"
    - "sync_logs articlesCreated and articlesUpdated counts are populated"
  artifacts:
    - path: "src/lib/ai/pipeline.ts"
      provides: "Top-level pipeline orchestrator: sync result -> articles"
      exports: ["runAIPipeline"]
    - path: "src/lib/github/sync.ts"
      provides: "Modified sync engine that triggers AI pipeline after file sync"
      exports: ["runSync", "ChangeSet"]
  key_links:
    - from: "src/lib/ai/pipeline.ts"
      to: "src/lib/ai/analyze.ts"
      via: "fetchFileContents + analyzeChanges for code analysis"
      pattern: "fetchFileContents|analyzeChanges"
    - from: "src/lib/ai/pipeline.ts"
      to: "src/lib/merge/three-way.ts"
      via: "mergeArticleContent for human-edited articles"
      pattern: "mergeArticleContent"
    - from: "src/lib/ai/pipeline.ts"
      to: "src/lib/merge/conflict.ts"
      via: "resolveConflict for conflict handling"
      pattern: "resolveConflict"
    - from: "src/lib/ai/pipeline.ts"
      to: "src/lib/content/version.ts"
      via: "createArticleVersion for version tracking"
      pattern: "createArticleVersion"
    - from: "src/lib/ai/pipeline.ts"
      to: "src/lib/content/markdown.ts"
      via: "markdownToBlocks for BlockNote JSON storage"
      pattern: "markdownToBlocks"
    - from: "src/lib/github/sync.ts"
      to: "src/lib/ai/pipeline.ts"
      via: "runAIPipeline called after applyChanges"
      pattern: "runAIPipeline"
---

<objective>
Build the pipeline orchestrator that wires AI analysis, article creation/update, merge strategy, and version tracking together, then integrate it into the sync flow so it runs automatically after every sync.

Purpose: This is the glue that makes the entire AI pipeline work end-to-end. After sync detects changes and stores file metadata, the pipeline fetches content, runs AI analysis, creates/updates articles with proper merge handling, populates file links and DB table mappings, and updates sync statistics.

Output: The pipeline orchestrator and modified sync engine that triggers AI processing automatically.
</objective>

<execution_context>
@/Users/michael/.claude/get-shit-done/workflows/execute-plan.md
@/Users/michael/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-ai-processing-pipeline/03-RESEARCH.md
@.planning/phases/03-ai-processing-pipeline/03-01-SUMMARY.md
@.planning/phases/03-ai-processing-pipeline/03-02-SUMMARY.md
@src/lib/db/schema.ts
@src/lib/github/sync.ts
@src/lib/github/client.ts
@src/lib/settings/constants.ts
@src/app/api/admin/sync/route.ts
@src/app/api/admin/sync/cron/route.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pipeline orchestrator</name>
  <files>src/lib/ai/pipeline.ts</files>
  <action>
**src/lib/ai/pipeline.ts** -- Top-level AI pipeline:

Import from plan 03-01: `fetchFileContents`, `analyzeChanges`, `getFullCategoryTree`, `getArticleIndex`, `generateArticle` from `@/lib/ai/analyze` and `@/lib/ai/generate`.
Import from plan 03-02: `mergeArticleContent` from `@/lib/merge/three-way`, `resolveConflict` from `@/lib/merge/conflict`, `createArticleVersion`, `getLastAIVersion` from `@/lib/content/version`, `blocksToMarkdown`, `markdownToBlocks` from `@/lib/content/markdown`, `generateUnifiedDiff` from `@/lib/merge/diff`.
Import DB: `getDb` from `@/lib/db`, schema tables (`articles`, `categories`, `articleFileLinks`, `articleDbTables`, `githubFiles`, `syncLogs`) from `@/lib/db/schema`, `eq`, `and`, `sql` from `drizzle-orm`.
Import settings: `getSetting` from `@/lib/settings`, `SETTING_KEYS` from `@/lib/settings/constants`.

Export `ChangeSet` type re-export from `@/lib/github/sync` for convenience.

Export `async function runAIPipeline(syncLogId: string, changedFilePaths: string[]): Promise<{ articlesCreated: number; articlesUpdated: number }>`:

1. **Early exit check:** If `changedFilePaths` is empty, return `{ articlesCreated: 0, articlesUpdated: 0 }`.

2. **Fetch file contents** from GitHub: call `fetchFileContents(changedFilePaths)`. If no contents returned (all files deleted or errored), return early.

3. **Build context:** Call `getFullCategoryTree()` and `getArticleIndex()` in parallel.

4. **Read prompts:** Get `analysis_prompt` and `article_style_prompt` from site_settings via `getSetting()`. Use sensible defaults if not configured (empty string fallback is fine -- the schema has defaults).

5. **Run AI analysis:** Call `analyzeChanges(fileContents, categories, articleIndex, analysisPrompt, stylePrompt)`. This returns a structured `AnalysisResponse` with articles to create/update.

6. **Process each article** in the analysis response:

   **For action = "create":**
   a. Resolve or create the category: Look up `category_suggestion` in existing categories by slug or name. If not found, create a new category in the `categories` table with a slug derived from the name (lowercase, hyphenated). Log when creating new categories.
   b. Generate full article content if needed: If `content_markdown` is short (<100 chars), call `generateArticle()` for a full body.
   c. Convert markdown to BlockNote JSON: `markdownToBlocks(contentMarkdown)`.
   d. Insert into `articles` table: title, slug (generate from title if not unique, append suffix), contentMarkdown, contentJson (BlockNote JSON), technicalViewMarkdown, categoryId, lastAiGeneratedAt = now, hasHumanEdits = false, needsReview = false.
   e. Create initial version: `createArticleVersion({ articleId, contentMarkdown, contentJson, technicalViewMarkdown, changeSource: "ai_generated", changeSummary })`.
   f. Populate `article_file_links`: For each `related_files` path, look up the `githubFiles` record by filePath. If found, insert into `articleFileLinks` with `relevanceExplanation` (derive from change_summary or use a generic explanation).
   g. Populate `article_db_tables`: For each `related_db_tables` entry, insert into `articleDbTables` with tableName, columns (as jsonb), relevanceExplanation.
   h. Increment `articlesCreated` counter.

   **For action = "update":**
   a. Look up existing article by slug. If not found, treat as "create" (AI suggested update for non-existent article).
   b. Check `has_human_edits` flag:
      - If FALSE (AI-only article): Overwrite directly. Update `contentMarkdown`, generate new `contentJson` via `markdownToBlocks()`, update `technicalViewMarkdown`, set `lastAiGeneratedAt = now`. Create version with `changeSource: "ai_updated"`.
      - If TRUE (human-edited article): Run merge strategy:
        i. Get last AI version via `getLastAIVersion(articleId)`. If null, use empty string as base (first AI update after human edits).
        ii. Get current article content (the human-edited version).
        iii. Convert current `contentJson` to Markdown via `blocksToMarkdown()` if contentJson exists, otherwise use `contentMarkdown`.
        iv. Run `mergeArticleContent(base: lastAIVersion.contentMarkdown, current: currentMarkdown, incoming: newAIContent)`.
        v. Call `resolveConflict()` with the merge result.
        vi. Update article: set `contentMarkdown` to `finalMarkdown`, generate `contentJson` via `markdownToBlocks()`, update `technicalViewMarkdown`, set `lastAiGeneratedAt = now`. If `needsReview`, set on article.
        vii. Create version with the appropriate `changeSource` from the conflict resolution.
   c. Update `article_file_links`: Delete existing links for this article, re-insert from AI response (simpler than diffing).
   d. Update `article_db_tables`: Same delete-and-reinsert pattern.
   e. Increment `articlesUpdated` counter.

7. **Update sync_logs** with article counts: Update the syncLogId row with `articlesCreated` and `articlesUpdated`.

8. Return `{ articlesCreated, articlesUpdated }`.

**Error handling:** Wrap individual article processing in try/catch. If one article fails, log the error and continue with the next. Don't let a single article failure abort the entire pipeline. Collect errors and include in sync_logs errorMessage if any occurred.

**Slug generation helper:** Create a private helper `generateSlug(title: string): string` that lowercases, replaces spaces/special chars with hyphens, and trims. For uniqueness, if slug exists in DB, append `-2`, `-3`, etc.
  </action>
  <verify>
`npx tsc --noEmit` passes. `runAIPipeline` correctly imports from all plan 03-01 and 03-02 modules. The function handles both "create" and "update" paths with proper merge strategy for human-edited articles.
  </verify>
  <done>
`runAIPipeline()` orchestrates the full flow: fetch content, analyze, create/update articles with merge strategy, populate file links and DB table mappings, track versions, and update sync statistics.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire pipeline into sync flow</name>
  <files>src/lib/github/sync.ts, src/app/api/admin/sync/route.ts, src/app/api/admin/sync/cron/route.ts</files>
  <action>
**Modify src/lib/github/sync.ts** -- Integrate AI pipeline into sync:

The key change: after `applyChanges(changeSet)` succeeds, trigger the AI pipeline BEFORE releasing the sync lock. This prevents race conditions (Pitfall 6 from research).

- Import `runAIPipeline` from `@/lib/ai/pipeline`.
- In `runSync()`, after step 6 (applyChanges), add step 6.5:
  ```
  // 6.5. Run AI pipeline on changed files (if any changes detected)
  const changedFilePaths = [
    ...changeSet.added.map(f => f.path),
    ...changeSet.modified.map(f => f.path),
  ];
  let aiStats = { articlesCreated: 0, articlesUpdated: 0 };
  if (changedFilePaths.length > 0) {
    try {
      aiStats = await runAIPipeline(syncLogId, changedFilePaths);
    } catch (error) {
      // AI pipeline failure should NOT fail the sync -- log and continue
      console.error("[sync] AI pipeline error:", error);
      // Optionally append to error tracking but don't throw
    }
  }
  ```
- Update the stats object to include `articlesCreated` and `articlesUpdated` from aiStats.
- Update `releaseSyncLock()` call to pass article counts.
- Modify `releaseSyncLock()` function to accept and persist `articlesCreated` and `articlesUpdated`:
  - Add these to the `set` object in the DB update: `articlesCreated: stats.articlesCreated ?? 0, articlesUpdated: stats.articlesUpdated ?? 0`.
- Update the `SyncResult` interface to include `articlesCreated` and `articlesUpdated` in `stats`.
- Export `ChangeSet` type (already exported, verify).

**Modify src/app/api/admin/sync/route.ts** -- Include AI stats in response:
- The response already returns `stats` from `runSync()`. Since we updated `SyncResult.stats` to include article counts, the response will automatically include `articlesCreated` and `articlesUpdated`. No structural changes needed beyond verifying the type flows through.

**Modify src/app/api/admin/sync/cron/route.ts** -- Same as above:
- The cron endpoint already returns `stats` from `runSync()`. Article counts will flow through automatically.
- No structural changes needed.

Both API routes should be read and verified to ensure they pass through the new stats fields. If they destructure or filter stats, update them to include the new fields.
  </action>
  <verify>
`npm run build` succeeds. The sync flow now: (1) acquires lock, (2) fetches tree, (3) detects changes, (4) applies file metadata, (5) runs AI pipeline, (6) releases lock with full stats. `SyncResult.stats` includes `articlesCreated` and `articlesUpdated`. Both API routes return the new fields.
  </verify>
  <done>
The sync engine triggers the AI pipeline automatically after every sync. The sync lock is held throughout AI processing (preventing race conditions). Sync statistics include article creation/update counts. Both manual and cron sync endpoints return the enhanced stats.
  </done>
</task>

</tasks>

<verification>
- `npm run build` succeeds with no type errors
- `src/lib/ai/pipeline.ts` exists and exports `runAIPipeline`
- `src/lib/github/sync.ts` calls `runAIPipeline` after `applyChanges`
- Sync lock held until AI pipeline completes (no release between sync and AI processing)
- `releaseSyncLock` persists `articlesCreated` and `articlesUpdated`
- Pipeline handles both "create" (new articles) and "update" (existing articles) paths
- Update path checks `has_human_edits` and uses merge strategy accordingly
- Category creation only happens when no existing category matches
- Article file links populated from AI analysis `related_files`
- Article DB table mappings populated from AI analysis `related_db_tables`
- Individual article failures don't abort the entire pipeline
</verification>

<success_criteria>
The full end-to-end flow works: sync detects changes -> AI analyzes code -> articles are created/updated with proper merge handling -> file links and DB mappings populated -> versions tracked -> sync stats include article counts. All 8 AIPL requirements (AIPL-01 through AIPL-08) are covered.
</success_criteria>

<output>
After completion, create `.planning/phases/03-ai-processing-pipeline/03-03-SUMMARY.md`
</output>
